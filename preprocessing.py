import os
import pickle

import random

import spacy

import numpy as np

### This program applies the preprocessing step to the prepared data generated by load_dataset.py. It can take time to process, but it has to be run only once.

## Load prepared dataset

directory = "dataset"

french_filename = os.path.join(directory, 'french_prepared.pkl')
english_filename = os.path.join(directory, 'english_prepared.pkl')

with open(french_filename, 'rb') as f:
    french_data = pickle.load(f)

with open(english_filename, 'rb') as f:
    english_data = pickle.load(f)

## Verify dataset

l = len(french_data)
assert l == len(english_data)

print(f"\n{l} sentences, example:")

i = random.randint(0,l-1)

print(french_data[i])
print(english_data[i])

## Tokenize sentences

# python -m spacy download fr_core_news_sm
fr = spacy.load("fr_core_news_sm") # load the French model to tokenize French text

# python -m spacy download en_core_web_sm
eng = spacy.load("en_core_web_sm") # load the English model to tokenize English text

def tokenize(text, french=True):
    """
    Tokenize a French (if french=True) or English (if french=False) text and return a list of tokens
    """
    if french:
        return [token.text for token in fr.tokenizer(text)]
    else:
        return [token.text for token in eng.tokenizer(text)]

print("\nTokenized versions:")

print(tokenize(french_data[i], french=True))
print(tokenize(english_data[i], french=False))

## Build vocabulary

def buildVocab(data, french=True):
    """
    Build and return a vocabulary (dictionary of distinct tokens) based on a list of sentences in a given language (French if french=True, else English)
    """
    vocabulary = {'pad':0, 'unk':1}      # 'pad' and 'unk' being special tokens, we put it at the beginning of our vocabulary. 'pad' refers to the padding added at the end of sentences while 'unk' refers to the words that were removed from the vocabulary.
    index = 1
    for sentence in data:
        for token in tokenize(sentence, french):
            if not token in vocabulary:     # add the new token in the vocabulary
                vocabulary[token] = index
                index += 1
    return vocabulary

french_vocab = buildVocab(french_data, french=True)

print("\nFrench vocabulary:")
print(f"{len(french_vocab)} tokens, beginning:")
print({k: v for i, (k, v) in enumerate(french_vocab.items()) if i < 20})

english_vocab = buildVocab(english_data, french=False)

print("\nEnglish vocabulary:")
print(f"{len(english_vocab)} tokens, beginning:")
print({k: v for i, (k, v) in enumerate(english_vocab.items()) if i < 20})

## Save vocabulary

with open(os.path.join(directory, 'french_vocab.pkl'), 'wb') as f:
    pickle.dump(french_vocab, f)

with open(os.path.join(directory, 'english_vocab.pkl'), 'wb') as f:
    pickle.dump(english_vocab, f)

print("\nVocabularies saved.")

## Numericalize sentences

def numericalize(data, vocab, french=True):
    """
    Replace each token by its index within a list of sentences in a given language (French if french=True, else English)
    """
    new_data = [[] for _ in range(len(data))]
    for n in range(len(data)):
        sentence = data[n]
        for token in tokenize(sentence, french):
            new_data[n].append(vocab[token])
    return new_data

french_data_num = numericalize(french_data, french_vocab, french=True)
english_data_num = numericalize(english_data, english_vocab, french=False)

print("\nNumericalized versions:")

print(french_data_num[i])
print(english_data_num[i])

## Harmonize sentence lengths

max_length = max( max([len(sentence) for sentence in french_data_num]), max([len(sentence) for sentence in english_data_num]) )  # length of the longest sentence in both datasets
print(f"\nHighest sentence length: {max_length}")

fr_percentile_99 = int(np.percentile([len(sentence) for sentence in french_data_num], 99))
en_percentile_99 = int(np.percentile([len(sentence) for sentence in english_data_num], 99))
max_length = max(fr_percentile_99, en_percentile_99)
print(f"Limit sentence length (top 99%): {max_length}")

def removeLongestSentences(fr_data, en_data, max_len):
    """
    Remove the sentences that are longer than max_len from their dataset, along with their translation in the other dataset
    """
    assert len(fr_data) == len(en_data)
    fr_filtered = []
    en_filtered = []
    for fr_sent, en_sent in zip(fr_data, en_data):
        if len(fr_sent) <= max_len and len(en_sent) <= max_len:
            fr_filtered.append(fr_sent)
            en_filtered.append(en_sent)
    return fr_filtered, en_filtered

french_data_num, english_data_num = removeLongestSentences(french_data_num, english_data_num, max_length)

print(f"\nTop 1% longest sentences removed. {len(french_data_num)} remaining sentences.")

def applyPadding(fr_data, en_data, max_len):
    """
    Add padding tokens at the end of the sentences in both datasets so that all of them have the same length
    """
    assert len(fr_data) == len(en_data)
    for n in range(len(fr_data)):
        fr_sentence, en_sentence = fr_data[n], en_data[n]
        fr_sentence += [0] * (max_len - len(fr_sentence))
        en_sentence += [0] * (max_len - len(en_sentence))
        assert len(fr_sentence) == max_len and len(en_sentence) == max_len
    return fr_data, en_data

french_data_num, english_data_num = applyPadding(french_data_num, english_data_num, max_length)

print(f"Padding added to all sentences with less than {max_length} words. Example:")

i = random.randint(0, len(french_data_num)-1)

print(french_data_num[i])
print(english_data_num[i])

## Save numericalized sentences

with open(os.path.join(directory, 'french_num.pkl'), 'wb') as f:
    pickle.dump(french_data_num, f)

with open(os.path.join(directory, 'english_num.pkl'), 'wb') as f:
    pickle.dump(english_data_num, f)

print("\nNumericalized datasets saved.")